import torch

from ..Archetypes import Transformer
from torch import Tensor

#-------------------------------

class Affine(Transformer):
    """
    Definition of the affine transformer as a basic linear transformation
    """
    def __init__(self):
        """
        Constructor

        Defines the affine coupling transformation as:
            .. math::
                z'_i = exp(h_0^i) * z_i + h_1^i
        So that both log det and inverse are analitically known, also the number of parameters needed for variable dimension is 2.
        """
        super().__init__(2)

    def forward(self, z: Tensor, h: Tensor) -> Tensor:
        """
        Override of the torch.nn.Module method

        Parameters
        ----------
        z
            Input variable
        h
            Parameters of the transformation

        Returns
        -------
        Tensor
            Output variable
        """
        return torch.exp(h[:, ::2]) * z + h[:, 1::2]

    def inverse(self, z: Tensor, h: Tensor) -> Tensor:
        """
        Inverse transformation evaluated through:
            .. math::
                z_i = (z_i' - h_1^i) * exp(-h_0^i)     

        Parameters
        ----------
        z
            Transformed variable
        h
            Parameters that defines the transformation

        Returns
        -------
        Tensor
            Untransformed variable
        """
        return (z - h[:, 1]) * torch.exp(-h[:, 0])

    def log_det(self, _: Tensor, h: Tensor) -> Tensor:
        r"""
        Compute the log determinant of the Jacobian which ends up in being simply:
            .. math::
                \log\det J = \sum_i h_0^i

        Parameters
        ----------
        _
            Input variable, not use in this case
        h
            Parameter list evaluated using the input variable

        Returns
        -------
        Tensor
            Log determinant
        """
        return torch.sum(h[:, ::2], dim = 1)

# class Affine(Flow):
#     """
#     Affine transformer in the most basic implementation possible
#     """
#     def __init__(self, conditioner: Conditioner):
#         r"""
#         Constructor
#
#         Takes as input a conditioner that creates the set of parameters to use inside the transformer that in this case are two parameter per random variable having that the transformation used is:
#             .. math::
#                 z'_i = \exp(\alpha_i) * z_i + \beta_i
#         so that the output of the conditioner needs to have the form $[\alpha_1, \beta_1, \alpha_2, \beta_2, \dots]$
#
#         Parameters
#         ----------
#         conditioner
#             Conditioner to use in the computations
#
#         Raises
#         ------
#         ValueError:
#             A control is done on the variable _trans_features to see if it's equal to 2
#         """
#         super().__init__()
#        
#         if conditioner.trans_features != 2:
#             raise ValueError("The number of parameters per input dimension generated by the conditioner is different!")
#
#         self._cond = conditioner
#
#
#     def forward(self, z: Tensor) -> tuple[Tensor, Tensor]:
#         """
#         Override of the torch.nn.Module method
#
#         Parameters
#         ----------
#         z
#             Input variable
#
#         Returns
#         -------
#         tuple[Tensor, Tensor]
#             Transformed variable along with the log determinant of the Jacobian of the transformation
#         """
#         h = self._cond(z)
#       
#         return torch.exp(h[:, ::2]) * z + h[:, 1::2], torch.sum(h[:, ::2], 1)
#
#
#     def inverse(self, z1: Tensor) -> tuple[Tensor, Tensor]:
#         r"""
#         Compute the inverse of the transformation using the analitical formula, which unfortunatly is recursive in a general way. In particular the transformation is:
#             .. math::
#                 z_i = \frac{z'_i - \beta_i}{\exp(\alpha_i)}, \hspace{2cm} {\alpha_i, \beta_i} = c_i(z_{<i})
#
#         Parameters
#         ----------
#         z1
#             Transformed variable
#
#         Returns
#         -------
#         tuple[Tensor, Tensor]
#             Untransformed variabel along with the log determinant of the inverse transformation
#         """ 
#         h = self._cond(torch.clone(z1))
#         z1[:, 0] = (z1[:,0] - h[:,1])/torch.exp(h[:,0])
#
#         for i in range(1, z1.shape[1]):
#             h = self._cond(torch.clone(z1))
#             z1[:,i] = (z1[:,i] - h[:, 2*i+1])/torch.exp(h[:, 2*i])
#
#         return z1, -torch.sum(self._cond(z1)[:, ::2], dim=1)
